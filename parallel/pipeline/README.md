# Pipeline

`Pipeline` позволяет разделить сложную задачу на несколько простых. Входные данные для одной подзадачи становятся выходными для другой, и этот процесс продолжается до тех пор, пока вся цепочка не завершится. Такой подход помогает разбивать обработку данных на этапы и эффективно управлять потоками данных.

посмотри, как это работает без использования горутин:

```go

func main() {
    value := 1

    // пример 1: сложение, затем умножение
    fmt.Println(multiply(add(value, 2), 3))

    // пример 2: поменяли местами этапы
    fmt.Println(add(multiply(value, 2), 3))
}

// add — функция сложения
func add(a, b int) int {
    return a + b
}

// multiply — функция умножения
func multiply(a, b int) int {
    return a * b
}
```

Как ты заметил, все операции выполняются последовательно. Один этап начинается только тогда, когда предыдущий закончил обработку всех данных. Однако, используя горутины, можно обработать данные параллельно, ускоряя выполнение.

Теперь я добавлю горутины и каналы, чтобы сделать наш конвейер более эффективным. Изменим функции `add`, чтобы они работали с каналами:

```go
// add — добавляет 2 к каждому значению из inputCh и возвращает канал с результатами
func add(doneCh chan struct{}, inputCh chan int) chan int {
    resultCh := make(chan int)

    go func() {
        defer close(resultCh)

        for value := range inputCh {
            result := value + 2

            select {
            case <-doneCh: // если нужно завершить горутину
                return
            case resultCh <- result: // отправляем результат
            }
        }
    }()

    return resultCh
}
```

Таким образом, когда ты передаешь числа 1, 2, 3 в `пайплайн`, данные проходят через несколько этапов параллельной обработки. Например, пока одно число умножается, другое уже может обрабатываться на этапе сложения. Это позволяет распараллеливать работу над большими данными без значительной нагрузки на 1 CP.
